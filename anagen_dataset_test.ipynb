{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import GPT2Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0314 10:48:17.135134 140524966041408 tokenization_utils.py:312] Model name '/home/hansonlu/links/data/anagen_models/anagen_anaph_only_b28_lr_default' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/home/hansonlu/links/data/anagen_models/anagen_anaph_only_b28_lr_default' is a path or url to a directory containing tokenizer files.\n",
      "I0314 10:48:17.165753 140524966041408 tokenization_utils.py:377] loading file /home/hansonlu/links/data/anagen_models/anagen_anaph_only_b28_lr_default/vocab.json\n",
      "I0314 10:48:17.166485 140524966041408 tokenization_utils.py:377] loading file /home/hansonlu/links/data/anagen_models/anagen_anaph_only_b28_lr_default/merges.txt\n",
      "I0314 10:48:17.167036 140524966041408 tokenization_utils.py:377] loading file /home/hansonlu/links/data/anagen_models/anagen_anaph_only_b28_lr_default/added_tokens.json\n",
      "I0314 10:48:17.167598 140524966041408 tokenization_utils.py:377] loading file /home/hansonlu/links/data/anagen_models/anagen_anaph_only_b28_lr_default/special_tokens_map.json\n",
      "I0314 10:48:17.168075 140524966041408 tokenization_utils.py:377] loading file /home/hansonlu/links/data/anagen_models/anagen_anaph_only_b28_lr_default/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "jsonlines_file = \"data/dev.english.256.head1.jsonlines\"\n",
    "with open(jsonlines_file, \"r\") as f:\n",
    "    l = f.readline()\n",
    "    \n",
    "tokenizer_dir = \"/home/hansonlu/links/data/anagen_models/anagen_anaph_only_b28_lr_default\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_index_remap(tokens_with_tags):\n",
    "    remap_idx = 0\n",
    "    res = []\n",
    "    for t in tokens_with_tags:\n",
    "        res.append(remap_idx)\n",
    "        if t != \"[CLS]\" and t != \"[SEP]\":\n",
    "            remap_idx += 1\n",
    "    return res\n",
    "\n",
    "def process_jsonline(line):\n",
    "    example = json.loads(line)\n",
    "    tokens_with_tags = flatten(example[\"sentences\"])\n",
    "    token_idx_remap = get_index_remap(tokens_with_tags)\n",
    "    tokens_without_tags = [t for t in tokens_with_tags if t != \"[CLS]\" and t != \"[SEP]\"]\n",
    "    raw_string_without_tags = \" \".join(tokens_without_tags)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try construct dataset directly from ontonotes data\n",
    "\n",
    "Try to copy and understand code in minimize.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import collections\n",
    "import conll\n",
    "from collections import defaultdict as DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"main\"\n",
    "data_dir = \"/home/hansonlu/links/data/coref_data\"\n",
    "do_lower_case = False\n",
    "seg_len = 256\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentState(object):\n",
    "    def __init__(self, key):\n",
    "        self.doc_key = key\n",
    "        self.sentence_end = []\n",
    "        self.token_end = []\n",
    "        self.tokens = []\n",
    "        self.subtokens = []\n",
    "        self.info = []\n",
    "        self.segments = []\n",
    "        self.subtoken_map = []\n",
    "        self.segment_subtoken_map = []\n",
    "        self.sentence_map = []\n",
    "        self.pronouns = []\n",
    "        self.clusters = collections.defaultdict(list)\n",
    "        self.coref_stacks = collections.defaultdict(list)\n",
    "        self.speakers = []\n",
    "        self.segment_info = []\n",
    "\n",
    "    def finalize(self):\n",
    "        # finalized: segments, segment_subtoken_map\n",
    "        # populate speakers from info\n",
    "        subtoken_idx = 0\n",
    "        for segment in self.segment_info:\n",
    "            speakers = []\n",
    "            for i, tok_info in enumerate(segment):\n",
    "                if tok_info is None and (i == 0 or i == len(segment) - 1):\n",
    "                    speakers.append('[SPL]')\n",
    "                elif tok_info is None:\n",
    "                    speakers.append(speakers[-1])\n",
    "                else:\n",
    "                    speakers.append(tok_info[9])\n",
    "                    if tok_info[4] == 'PRP':\n",
    "                        self.pronouns.append(subtoken_idx)\n",
    "                subtoken_idx += 1\n",
    "            self.speakers += [speakers]\n",
    "        # populate sentence map\n",
    "\n",
    "        # populate clusters\n",
    "        first_subtoken_index = -1\n",
    "        for seg_idx, segment in enumerate(self.segment_info):\n",
    "            speakers = []\n",
    "            for i, tok_info in enumerate(segment):\n",
    "                first_subtoken_index += 1\n",
    "                coref = tok_info[-2] if tok_info is not None else '-'\n",
    "                if coref != \"-\":\n",
    "                    last_subtoken_index = first_subtoken_index + tok_info[-1] - 1\n",
    "                    for part in coref.split(\"|\"):\n",
    "                        if part[0] == \"(\":\n",
    "                            if part[-1] == \")\":\n",
    "                                cluster_id = int(part[1:-1])\n",
    "                                self.clusters[cluster_id].append((first_subtoken_index, last_subtoken_index))\n",
    "                            else:\n",
    "                                cluster_id = int(part[1:])\n",
    "                                self.coref_stacks[cluster_id].append(first_subtoken_index)\n",
    "                        else:\n",
    "                            cluster_id = int(part[:-1])\n",
    "                            start = self.coref_stacks[cluster_id].pop()\n",
    "                            self.clusters[cluster_id].append((start, last_subtoken_index))\n",
    "        # merge clusters\n",
    "        merged_clusters = []\n",
    "        for c1 in self.clusters.values():\n",
    "            existing = None\n",
    "            for m in c1:\n",
    "                for c2 in merged_clusters:\n",
    "                    if m in c2:\n",
    "                        existing = c2\n",
    "                        break\n",
    "                if existing is not None:\n",
    "                    break\n",
    "            if existing is not None:\n",
    "                print(\"Merging clusters (shouldn't happen very often.)\")\n",
    "                existing.update(c1)\n",
    "            else:\n",
    "                merged_clusters.append(set(c1))\n",
    "        merged_clusters = [list(c) for c in merged_clusters]\n",
    "        all_mentions = util.flatten(merged_clusters)\n",
    "        sentence_map =  get_sentence_map(self.segments, self.sentence_end)\n",
    "        subtoken_map = util.flatten(self.segment_subtoken_map)\n",
    "        assert len(all_mentions) == len(set(all_mentions))\n",
    "        num_words =  len(util.flatten(self.segments))\n",
    "        assert num_words == len(util.flatten(self.speakers))\n",
    "        assert num_words == len(subtoken_map), (num_words, len(subtoken_map))\n",
    "        assert num_words == len(sentence_map), (num_words, len(sentence_map))\n",
    "        return {\n",
    "            \"doc_key\": self.doc_key,\n",
    "            \"sentences\": self.segments,\n",
    "            \"speakers\": self.speakers,\n",
    "            \"constituents\": [],\n",
    "            \"ner\": [],\n",
    "            \"clusters\": merged_clusters,\n",
    "            'sentence_map':sentence_map,\n",
    "            \"subtoken_map\": subtoken_map,\n",
    "            'pronouns': self.pronouns\n",
    "        }\n",
    "\n",
    "def normalize_word(word, language):\n",
    "    if language == \"arabic\":\n",
    "        word = word[:word.find(\"#\")]\n",
    "    if word == \"/.\" or word == \"/?\":\n",
    "        return word[1:]\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def get_document(document_lines, tokenizer, language, segment_len):\n",
    "    document_state = DocumentState(document_lines[0])\n",
    "    word_idx = -1\n",
    "    for line in document_lines[1]:\n",
    "        row = line.split()\n",
    "        sentence_end = len(row) == 0\n",
    "        if not sentence_end:\n",
    "            assert len(row) >= 12\n",
    "            word_idx += 1\n",
    "            word = normalize_word(row[3], language)\n",
    "            subtokens = tokenizer.tokenize(word)\n",
    "            document_state.tokens.append(word)\n",
    "            document_state.token_end += ([False] * (len(subtokens) - 1)) + [True]\n",
    "            for sidx, subtoken in enumerate(subtokens):\n",
    "                document_state.subtokens.append(subtoken)\n",
    "                info = None if sidx != 0 else (row + [len(subtokens)])\n",
    "                document_state.info.append(info)\n",
    "                document_state.sentence_end.append(False)\n",
    "                document_state.subtoken_map.append(word_idx)\n",
    "        else:\n",
    "            document_state.sentence_end[-1] = True\n",
    "    # split_into_segments(document_state, segment_len, document_state.token_end)\n",
    "    # split_into_segments(document_state, segment_len, document_state.sentence_end)\n",
    "    constraints1 = document_state.sentence_end if language != 'arabic' else document_state.token_end\n",
    "    split_into_segments(document_state, segment_len, constraints1, document_state.token_end)\n",
    "    stats[\"max_sent_len_{}\".format(language)] = max(max([len(s) for s in document_state.segments]), stats[\"max_sent_len_{}\".format(language)])\n",
    "    document = document_state.finalize()\n",
    "    return document\n",
    "\n",
    "\n",
    "def minimize_partition(tokenizer, name=\"dev\", language=\"english\", extension=\"v4_gold_conll\", seg_len=256):\n",
    "    # (original) input_path = ~/links/data/coref_data/dev.english.v4_gold_conll\n",
    "    # input_path = \"{}/{}.{}.{}\".format(input_dir, name, language, extension)\n",
    "    input_path = \"/home/hansonlu/anagen/coref/data/dev.english.twodoc.v4_gold_conll\"\n",
    "#     output_path = \"{}/{}.{}.{}.jsonlines\".format(output_dir, name, language, seg_len)\n",
    "    count = 0\n",
    "    print(\"Minimizing {}\".format(input_path))\n",
    "    documents = []\n",
    "    with open(input_path, \"r\") as input_file:\n",
    "        for line in input_file.readlines():\n",
    "            begin_document_match = re.match(conll.BEGIN_DOCUMENT_REGEX, line)\n",
    "            if begin_document_match:\n",
    "                doc_key = conll.get_doc_key(begin_document_match.group(1), begin_document_match.group(2))\n",
    "                documents.append((doc_key, []))\n",
    "            elif line.startswith(\"#end document\"):\n",
    "                continue\n",
    "            else:\n",
    "                documents[-1][1].append(line)\n",
    "    # documents: List[Tuple[str: doc_key, List[str]]\n",
    "    \n",
    "    document_line = get_document(document)\n",
    "    return\n",
    "#     with open(output_path, \"w\") as output_file:\n",
    "#         for document_lines in documents:\n",
    "#             if skip(document_lines[0]):\n",
    "#                 continue\n",
    "#             document = get_document(document_lines, tokenizer, language, seg_len)\n",
    "#             output_file.write(json.dumps(document))\n",
    "#             output_file.write(\"\\n\")\n",
    "#             count += 1\n",
    "#     print(\"Wrote {} documents to {}\".format(count, output_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimizing /home/hansonlu/anagen/coref/data/dev.english.twodoc.v4_gold_conll\n",
      "<class 'list'>\n",
      "540\n"
     ]
    }
   ],
   "source": [
    "minimize_partition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
